{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward Neural Networks\n",
    "In this note book we develop a system to recognize handwritten digit MNIST using feed-forward nets (FFN) a.k.a fully-connected-nets. The architecture of FFN with one hidden layer is illustrated below\n",
    "\n",
    "<img src=\"./assets/ffn_1h.png\" width=\"60%\">\n",
    "\n",
    "where in mathematics term we define\n",
    "* $W_{ih}\\in\\mathbb{R}^{784\\times H}$ is the weights from input-to-hidden\n",
    "* $b_{h}\\in\\mathbb{R}^{H}$ is the biases for hidden layer\n",
    "* $W_{ho}\\in\\mathbb{R}^{H\\times 10}$ is the weights from hidden to output\n",
    "* $b_{o}\\in\\mathbb{R}^{10}$ is the biases for output layer\n",
    "* $a_h(x)$ is activation at hidden layer\n",
    "* given an input $x$ (represented in row-vector form), the hidden layer $h$ and output layer $o$ are computed by following equation\n",
    "    $$\\left\\{\\begin{split} \n",
    "    h &= a_h\\left(x\\times W_{ih} + b_h\\right)\\\\\n",
    "    o &= \\mathrm{softmax}\\left(h\\times W_{ho} + b_o\\right)\n",
    "    \\end{split}\n",
    "    \\right.$$\n",
    "   where \n",
    "   $$\n",
    "   \\mathrm{softmax}\\left(v_1,\\ldots,v_n\\right) = \\left(\\frac{e^{v_1}}{\\sum_{i=1}^n e^{v_i}},\\ldots,\\frac{e^{v_n}}{\\sum_{i=1}^n e^{v_i}}\\right)\n",
    "   $$\n",
    "* given the label $y$, the cross-entropy loss is defined as\n",
    "$$\n",
    "-\\log(o_{y})\n",
    "$$\n",
    "we can see by minimize the loss function, it's equivalent to make $o_y$ closer to 1.\n",
    "\n",
    "The activation function can be one of the following forms\n",
    "* $a_h(x) = \\frac{1}{1+e^{-x}}$  is called sigmoid activation\n",
    "* $a_h(x) = \\tanh(x)$  is called tanh activation\n",
    "* $a_h(x) = \\max(x,0)$ is call ReLU activation\n",
    "\n",
    "## Implement FFN\n",
    "\n",
    "We can implement FFN from scratch but it's much easier to use Tensorflow. Furthermore Tensorflow also allows you to harvest the GPU power without event changing your code.\n",
    "\n",
    "We start by loading some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# import numpy, tensorflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist_data = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# helper function allow user to choose solver\n",
    "def get_solver(solver_name):\n",
    "    if solver_name == 'sgd':\n",
    "        return tf.train.GradientDescentOptimizer\n",
    "    elif solver_name == 'momentum':\n",
    "        return tf.train.MomentumOptimizer\n",
    "    elif solver_name == 'adam':\n",
    "        return tf.train.AdamOptimizer\n",
    "    else:\n",
    "        raise Exception('solver {} is not tested'.format(solver_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the high level package `layers` to craft our FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistFFn(object):\n",
    "    def __init__(self, num_hidden = 256, activation = tf.nn.sigmoid):\n",
    "        self._num_hidden = num_hidden\n",
    "        self._activation = activation\n",
    "        self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        self._add_placeholder()\n",
    "        self._build_net()\n",
    "        \n",
    "    def _add_placeholder(self):\n",
    "        with tf.variable_scope('mnist_input'):\n",
    "            self._x = tf.placeholder(tf.float32, [None, 784], name = 'images')\n",
    "            self._y = tf.placeholder(tf.int32,   [None, 10],  name = 'labels')\n",
    "    \n",
    "    def _build_net(self):\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        with tf.variable_scope('input_to_hidden'):\n",
    "            self._hiddens = tf.layers.dense(inputs = self._x, \n",
    "                                            units = self._num_hidden,\n",
    "                                            activation = self._activation,\n",
    "                                            kernel_initializer = xavier_init)\n",
    "        \n",
    "        with tf.variable_scope('hidden_to_output'):\n",
    "            \n",
    "            self._logits  = tf.layers.dense(inputs = self._hiddens,\n",
    "                                            units = 10,\n",
    "                                            activation = None,\n",
    "                                            kernel_initializer = xavier_init)\n",
    "            \n",
    "            self._loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits = self._logits,\n",
    "                                                                                 labels = self._y))\n",
    "        \n",
    "        with tf.variable_scope('mnist_eval'):\n",
    "            self._preds = tf.argmax(self._logits, axis=1)\n",
    "            self._truth = tf.argmax(self._y, axis = 1)\n",
    "            self._accuracy = tf.reduce_mean(tf.cast(tf.equal(self._preds, self._truth), tf.float32))\n",
    "            \n",
    "    def train(self, mnist_data, num_iters, batch_size=64, \n",
    "              solver = 'adam', learning_rate=1e-3, print_every=100):\n",
    "        with tf.Session() as sess:            \n",
    "            train_op = get_solver(solver)(learning_rate = learning_rate).minimize(self._loss)\n",
    "            \n",
    "            # init value\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(num_iters):\n",
    "                batch_xs, batch_ys = mnist_data.train.next_batch(batch_size)\n",
    "                loss,_ = sess.run([self._loss, train_op], feed_dict={self._x: batch_xs, self._y: batch_ys})\n",
    "                \n",
    "                sys.stdout.write(\"\\rIteration ({}/{})\".format(i + 1, num_iters)\n",
    "                                     + \"Loss {:.4f} \".format(loss))\n",
    "                if ((i+1)%print_every == 0 or i+1==num_iters):\n",
    "                    train_acc = sess.run(self._accuracy, {self._x: mnist_data.train.images,\n",
    "                                                          self._y: mnist_data.train.labels})\n",
    "                    acc = sess.run(self._accuracy, {self._x: mnist_data.validation.images,\n",
    "                                                    self._y: mnist_data.validation.labels})\n",
    "                    print('\\nTrain vs Validation accuracy {:.2f}% vs {:.2f}%\\n'.format(100*train_acc, \n",
    "                                                                                       100*acc))\n",
    "            \n",
    "            # evaluate on test set\n",
    "            acc = sess.run(self._accuracy, {self._x: mnist_data.test.images,\n",
    "                                            self._y: mnist_data.test.labels})\n",
    "            print('\\nTest-accuracy is {:.2f}%\\n'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our FFN on MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration (500/5000)Loss 0.1185 \n",
      "Train vs Validation accuracy 96.35% vs 96.26%\n",
      "\n",
      "Iteration (1000/5000)Loss 0.0607 \n",
      "Train vs Validation accuracy 98.03% vs 97.42%\n",
      "\n",
      "Iteration (1500/5000)Loss 0.0361 \n",
      "Train vs Validation accuracy 98.79% vs 97.64%\n",
      "\n",
      "Iteration (2000/5000)Loss 0.0292 \n",
      "Train vs Validation accuracy 99.36% vs 98.02%\n",
      "\n",
      "Iteration (2500/5000)Loss 0.0196 \n",
      "Train vs Validation accuracy 99.66% vs 97.76%\n",
      "\n",
      "Iteration (3000/5000)Loss 0.0084 \n",
      "Train vs Validation accuracy 99.80% vs 97.78%\n",
      "\n",
      "Iteration (3500/5000)Loss 0.0178 \n",
      "Train vs Validation accuracy 99.91% vs 98.06%\n",
      "\n",
      "Iteration (4000/5000)Loss 0.0033 \n",
      "Train vs Validation accuracy 99.97% vs 97.98%\n",
      "\n",
      "Iteration (4500/5000)Loss 0.0024 \n",
      "Train vs Validation accuracy 99.98% vs 98.18%\n",
      "\n",
      "Iteration (5000/5000)Loss 0.0034 \n",
      "Train vs Validation accuracy 99.97% vs 97.88%\n",
      "\n",
      "\n",
      "Test-accuracy is 97.86%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "ffn_model = MnistFFn(num_hidden=256, activation=tf.nn.relu)\n",
    "\n",
    "num_iters = 5000\n",
    "batch_size = 256\n",
    "\n",
    "ffn_model.train(mnist_data, num_iters, batch_size = batch_size, print_every=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
