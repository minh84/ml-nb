{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC\n",
    "-   [Introduction](#Introduction)\n",
    "-   [Supervised learning](#Supervised-learning)\n",
    "    -   [Linear regression](#Linear-regression)\n",
    "    -   [Binary classification](#Binary-classification)\n",
    "-   [Unsupervised learning](#Unsupervised-learning)    \n",
    "    -   [Restricted Boltzmann Machine](#Restricted-Boltzmann-Machine)\n",
    "    \n",
    "## Introduction\n",
    "Machine learning (from [wikipedia](https://en.wikipedia.org/wiki/Machine_learning)) is the subfield of computer science that, according to Arthur Samuel in 1959, gives \"computers the ability to learn without being explicitly programmed.\" Here learning means recognizing and understanding the input data then can make predictions on data. \n",
    "\n",
    "In this series of blogs, I would like to share with you a practical introduction to machine learning and statistical pattern recognition. This covers the three categories of machine learning\n",
    "\n",
    "\n",
    "![ml-categories](./assets/machine_learning-categories.jpg)\n",
    "\n",
    "## Supervised learning\n",
    "Supervised learning (from [wikipedia](https://en.wikipedia.org/wiki/Supervised_learning)) is the machine learning task of inferring a function from labeled training data. For example\n",
    "* given a training data contains the living areas and prices of some houses, we want to learn to predict the price of other houses in function of their living area.\n",
    "\n",
    "* given a training data contains emails and their labels (spam/non-spam), we want to learn to predict whether new incoming email is spam/non-spam\n",
    "\n",
    "Let's define some notation for future use\n",
    "\n",
    "* $x^{(i)}\\in\\mathcal{X}$ denote the `input` variables, also called input **features**\n",
    "* $y^{(i)}\\in\\mathcal{Y}$ denote the `output` or **target** variable that we want to predict\n",
    "* a pair $(x^{(i)}, y^{(i)})$ is called a **training example**\n",
    "* a list of $(x^{(i)}, y^{(i)}),i=1,\\ldots,m$ is called a **training set**\n",
    "\n",
    "Our goal, given a training set, is to learn a function $h:\\mathcal{X}\\mapsto\\mathcal{Y}$ so that $h(x)$ is a \"good\" predictor for the corresponding value of $y$. For historical reason, this function $h$ is call a **hypothesis**. The process is illustrated like this\n",
    "\n",
    "<img src=\"./assets/supervised-process.png\" alt=\"supervised-learning-process\" style=\"width: 300px;\"/>\n",
    "\n",
    "The **standard appoach** to supervised learning problems is\n",
    "\n",
    "* pick a representation for **hypothesis** function $h$\n",
    "* pick a **loss** function $L(h(x), y)$ that we will minimize\n",
    "\n",
    "The supervised learning can divided into two categories\n",
    "* when the target $y$ is continuous (e.g house price), we call it a **regression** problem\n",
    "* when the target $y$ can only take discrete values (e.g spam/non-spam), we call it a **classification** problem\n",
    "\n",
    "### Linear regression\n",
    "Let's consider the case $\\mathcal{X}=\\mathbb{R}^D, \\mathcal{Y}=\\mathbb{R}$ and a linear representation of the input for our **hypothesis**\n",
    "\n",
    "$$\n",
    "h(x,\\theta) = \\theta_0 + \\theta_1 x_1 +\\ldots+\\theta_Dx_D\n",
    "$$\n",
    "\n",
    "Then we pick least-square error as **loss** function\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m \\left(h(x^{(i)},\\theta) - y^{(i)}\\right)^2\n",
    "$$\n",
    "\n",
    "The linear regression is discussed in the following notebooks\n",
    "* Linear regression [part 1](./supervised/linear_regression_part01.ipynb) which covers \n",
    "    * probabilistic interpretation: **maximum-likelihood (ML)** \n",
    "    * iterative first order optimization algorithm: gradient descent\n",
    "* Linear regression [part 2](./supervised/linear_regression_part02.ipynb) which covers\n",
    "    * the normal equation: closed form for linear regression\n",
    "    * parameter regularization: Bayesian view and **maximum-a-posteriori (MAP)** \n",
    "\n",
    "### Binary classification\n",
    "Let's consider now the binary classification problem e.g filter an email is spam or non-spam, predict the weather is rain or dry e.t.c. \n",
    "\n",
    "#### Logistic regression\n",
    "One simple and very popular approach to the binary classification is **logistic regression** algorithm where we consider the **hypothesis** \n",
    "$$\n",
    "h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}\n",
    "$$\n",
    "The notebook on [logistic regression](./supervised/binary_classification_logistic.ipynb) will cover the following topics\n",
    "* Logistic regression theory and implementation\n",
    "* Generalized linear models\n",
    "\n",
    "## Unsupervised learning\n",
    "Unsupervised learning (from [wikipedia](https://en.wikipedia.org/wiki/Unsupervised_learning) is the machine learning task of inferring a function to describe hidden structure from \"unlabeled\" data. \n",
    "\n",
    "Approaches to unsupervised learning include:\n",
    "* clustering\n",
    "    * k-means\n",
    "* Neural Networks\n",
    "    * Restricted Boltzmann Machine\n",
    "    \n",
    "###  Restricted Boltzmann Machine   \n",
    "Restricted Boltzman Machine (RBM) is energy-based model which consists of a layer of visible units and a layer of hidden units with no visible-visible or hidden-hidden connections.\n",
    "\n",
    "The implementation of RBMs is done in [here](./unsupervised/rbm.ipynb) which covers     \n",
    "* Introduction to Restricted Boltzmann Machine (RBM)\n",
    "* RBM feature learning to aplly in MNIST classification task        \n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
