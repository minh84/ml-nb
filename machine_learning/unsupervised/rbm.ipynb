{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzmann Machine (RBM)\n",
    "A [RBM](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs. \n",
    "\n",
    "A standard RBM has following diagram\n",
    "<img src=\"../assets/rbm-diagram.png\" alt=\"rbm-diagram\" style=\"width: 40%; height: 40%\">\n",
    "\n",
    "where we denote\n",
    "* $v\\in\\mathbb{R}^D$ is visible units\n",
    "* $h\\in\\mathbb{R}^H$ is hidden units\n",
    "\n",
    "And $v,h$ takes binary value (0,1), then it defines an energy function (similar as Hopfield network)\n",
    "$$\n",
    "E(v,h) = -a^Tv -b^Th - v^TWh\n",
    "$$\n",
    "which allows us to model the joint-distribution $(v,h)$ in term of the energy function i.e\n",
    "$$\n",
    "P(v,h) = \\frac{1}{Z}e^{-E(v,h)}\n",
    "$$\n",
    "where $Z$ is normalized constant i.e\n",
    "$$\n",
    "Z = \\sum_{v,h}e^{-E(v,h)}\n",
    "$$\n",
    "\n",
    "The above diagram is a bipartite graph which allows to define conditional probability \n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "P(v|h) &= \\prod_{i=1}^DP(v_i|h)\\\\\n",
    "P(h|v) &= \\prod_{j=1}^HP(h_j|v)\n",
    "\\end{array}\n",
    "$$\n",
    "where individual probability is given by\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "P(v_i=1|h) &= \\sigma\\left(a_i + \\sum_{j=1}^{H}w_{i,j}h_j\\right)\\\\\n",
    "P(h_j=1|v) &= \\sigma\\left(b_j + \\sum_{i=1}^Dw_{i,j}v_i\\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In this note, we look at the following task\n",
    "* given a set of unlabelled dataset $\\left\\{v^{(i)}\\right\\}_{i=1}^n$\n",
    "* given a configuration of RMB e.g number of hidden units $H$\n",
    "\n",
    "The goal is to learn $a$, $b$ and $W$ that model the joint-distribution directly from dataset $\\left\\{v^{(i)}\\right\\}_{i=1}^n$ i.e that maximize the sum of log-likelihood\n",
    "$$\n",
    "\\sum_{i=1}^n\\log\\left(P(v^{(i)})\\right)\n",
    "$$\n",
    "where \n",
    "$$\n",
    "P(v) = \\sum_{h}P(v,h)=\\frac{1}{Z}\\sum_{h}e^{-E(v,h)}\n",
    "$$\n",
    "The learning for RBM is difficult since computation of $Z$ is exponentially hard, however we have the following interesting fact\n",
    "$$\n",
    "\\frac{\\partial \\log P(v)}{\\partial v} = <v_i,h_j>_{data} - <v_i,h_j>_{model}\n",
    "$$\n",
    "where\n",
    "* $<v_i, h_j>_{data}$ is expected value of the product when $v$ is clamped on data (i.e $v_i$ is taken from training dataset)\n",
    "* $<v_i, h_j>_{model}$ is expected value of the product when $v,h$ is generated by model\n",
    "\n",
    "Now let's look at how we compute $<v_i,h_j>_{data}$ and $<v_i,h_j>_{model}$, it's done via two phases\n",
    "* **positive phase**: clamp a datavector on the visible units, then we sample $h$ and compute the average $<v,h>$\n",
    "* **negative phase**: keep a set of fantasy particles $v$ where each particle has a value that is a global configuration, then we sample $h$ and compute the average $<v,h>$\n",
    "\n",
    "To illustrate the process, we show the following diagram is taken from G. Hinton's course (NN)\n",
    "<img src=\"../assets/rbm-pos-neg.png\" alt=\"rbm-pos-neg\" style=\"width: 60%; height: 50%\">\n",
    "\n",
    "and the update rule for $W$\n",
    "$$\n",
    "\\Delta w_{i,j} = \\epsilon\\left( <v_i,h_j>^0 - <v_i,h_j>^\\infty \\right)\n",
    "$$\n",
    "This is actually based on Markov Chain Monte Carlo (MCMC) and Gibbs sampling. However this's still not efficient enough. \n",
    "\n",
    "Fortunately, G. Hinton introduces a very fast learning algorithm so called **Constrative Divergence Learning** where we can use a short cut\n",
    "<img src=\"../assets/rbm-cd-learning.png\" alt=\"rbm-cd-learning\" style=\"width: 60%; height: 50%\">\n",
    "we denote \n",
    "$$\n",
    "CD_1 = <v_i,h_j>^0 - <v_i,h_j>^1\n",
    "$$\n",
    "we can similarly define $CD_t = <v_i,h_j>^0 - <v_i,h_j>^t$. One can see that the learning doesn't use the correct gradient of the log-likelihood, as suggested by G. Hinton, we can use the following process\n",
    "* start with small weight $w_{i,j}$ and we use $CD_1$\n",
    "* once the weights grow (after some number of iterations), we can use $CD_3$, then $CD_{10}$\n",
    "\n",
    "We will consider MNIST dataset for this task, then later we go through some RBM's application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
